{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cc0e29-abf0-4653-92ea-1ccf62d56e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -Uq diffusers transformers fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6657441-18fb-4d10-9c64-23469d533bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e587061cc94ed983f0517745b25624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "import logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc2b28e-ba1b-4b7f-87ca-bd6323f4e242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982f0879a1384358a10df3562542140d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# Log into hugging face\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# Scheduler Parameters\n",
    "beta_start, beta_end = 0.00085, 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761e2379-2a1e-425d-a32f-b0d0280e78b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f4ce447d9f4ff7b2bf637ee977eb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914e3c98e9ec49c7ac8fd067942b9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6083be6e8c14b03a58ab8335f361637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ain/unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8972abdf144942bbb2e6367e897e486a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16db0bb778a5446ebf7743e86b201fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c84528a64541c1912270c43bae56b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d39afc88f4a169703a61b0b6dc1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda96f895f0447578f821f0e407e234b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2e5a1954374d89bd0e4d3eb7461d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bca7a3419774c89b11b4dfb3400a543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28487b6b6aa443e89a72d6e9c67393fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import VAE, UNET, Scheduler, Tokenizer and Text Encoder\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a36498-c40b-46d6-946f-aae530e935c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prompt and Hyperparameters\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 70\n",
    "guidance_scale = 7.5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7a0863-6849-4287-bb59-cd2f583d8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_enc(prompts, maxlen=None):\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "def mk_img(t):\n",
    "    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((image*255).round().astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6363b38-c257-4ed5-9e33-489234bb3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_samples(prompts, neg_prompts=None, init_image=None, g=7.5, seed=100, steps=70):\n",
    "    bs = len(prompts)\n",
    "    text = text_enc(prompts)\n",
    "    if neg_prompts is None:\n",
    "        neg_prompts = text_enc([\"\"] * bs, text.shape[1])\n",
    "    else:\n",
    "        neg_prompts = text_enc(neg_prompts)\n",
    "    emb = torch.cat([neg_prompts, text])\n",
    "    if seed: torch.manual_seed(seed)\n",
    "\n",
    "    \n",
    "    latents = None\n",
    "    # I need to change the latents to be the latents of (init_image + noise)\n",
    "    if init_image is None:\n",
    "        latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((height, width)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        init_tensor_image = transform(init_image)\n",
    "        init_tensor_image = init_tensor_image.unsqueeze(0).to(\"cuda\").half()  \n",
    "        # print(f\"Actual: {init_tensor_image.shape}\")\n",
    "        latents = vae.encode(init_tensor_image)\n",
    "        # expected_latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n",
    "        # print(f\"Actual: {latents.shape}\")\n",
    "        # print(f\"Expected: {expected_latents.shape}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.set_timesteps(steps)\n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "\n",
    "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "        pred = u + g*(t-u)\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa3086f-1323-4d04-8652-c45006eef325",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['volcano with dinosaurs']\n",
    "neg_prompts = ['blue']\n",
    "init_image = Image.open('Children-draw.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc55f7c2-20d4-413c-bd8d-301a41cca476",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutoencoderKLOutput' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# With initial image\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmk_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images: display(mk_img(img))\n",
      "Cell \u001b[0;32mIn [7], line 34\u001b[0m, in \u001b[0;36mmk_samples\u001b[0;34m(prompts, neg_prompts, init_image, g, seed, steps)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# expected_latents = torch.randn((bs, unet.in_channels, height//8, width//8))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(f\"Actual: {latents.shape}\")\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print(f\"Expected: {expected_latents.shape}\")\u001b[39;00m\n\u001b[1;32m     33\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mset_timesteps(steps)\n\u001b[0;32m---> 34\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;241m*\u001b[39m scheduler\u001b[38;5;241m.\u001b[39minit_noise_sigma\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(scheduler\u001b[38;5;241m.\u001b[39mtimesteps)):\n\u001b[1;32m     37\u001b[0m     inp \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mscale_model_input(torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m), ts)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoencoderKLOutput' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# With initial image\n",
    "images = mk_samples(prompts, init_image = init_image)\n",
    "for img in images: display(mk_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c070bc-6122-41e4-ab90-c8ad463a89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With initial image\n",
    "images = mk_samples(prompts, init_image = init_image)\n",
    "for img in images: display(mk_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900d74a-0d54-463e-9014-60e864fcb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working mk_samples with neg_prompts\n",
    "\n",
    "def mk_samples(prompts, neg_prompts=None, g=7.5, seed=100, steps=70):\n",
    "    bs = len(prompts)\n",
    "    text = text_enc(prompts)\n",
    "    if neg_prompts is None:\n",
    "        neg_prompts = text_enc([\"\"] * bs, text.shape[1])\n",
    "    else:\n",
    "        neg_prompts = text_enc(neg_prompts)\n",
    "    emb = torch.cat([neg_prompts, text])\n",
    "    if seed: torch.manual_seed(seed)\n",
    "\n",
    "    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n",
    "    scheduler.set_timesteps(steps)\n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "\n",
    "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "        pred = u + g*(t-u)\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc1b84-0b1f-46d3-a990-89cfa8bcef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without initial image\n",
    "images = mk_samples(prompts, seed=44)\n",
    "for img in images: display(mk_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4686df-64e5-4e82-9e36-bba75dce875a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
